{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "\n",
    "### Source\n",
    "Both datasets were taken from the UCI Machine Learning Repository.  The HTRU2 dataset can be found at https://archive.ics.uci.edu/ml/datasets/HTRU2, and the Letter Recognition set can be found at https://archive.ics.uci.edu/ml/datasets/letter+recognition.\n",
    "\n",
    "### Data Dictionaries\n",
    "\n",
    " \n",
    "### Data Pre-processing\n",
    "#### Compiling Data Sources\n",
    "\n",
    "#### Missing Values\n",
    "\n",
    "\n",
    "#### Categorical Values\n",
    "\n",
    "### The Problems\n",
    "#### HTRU2\n",
    "The HTRU2 dataset posed the problem of classifying pulsar candidates as a positive case (legitimate pulsar) or a negative case (not a legitimate pulsar).\n",
    "\n",
    "#### Letter Recognition\n",
    "The Letter Recognition posed the problem of classifying records as one of the 26 capital letters in the English alphabet.\n",
    "\n",
    "### Why These Datasets?\n",
    "I chose these datasets because while both were used for classification problems, the datasets were drastically different.  The HTRU2 dataset contained a large number of records, and relatively low number of features.  In contrast, the Letter Recognition dataset contained many more features.  In addition, the problem for the HTRU2 dataset was binary classification while the problem for the Letter Recognition set was to group into 26 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "The first algorithm used to model the two datasets was a decision tree.  For each dataset, 70% of the records were selected randomly as a training set, and the remaining 30% were held out as a test set to be only used once the models were finished.  One interesting hyperparameter in a decision tree model is the max depth of the tree.  As a decision tree is created to fit training data, the tree can be allowed to grow large enough to perfectly classify the data, unless there are some anomolies (for example two records with identical attributes, but different classification).  As a result, the tree can become so dependant on the training records selected that its ability to model new data points decreases.  This is a result of the high variance created by allowing a tree to grow too large.  However, if a tree is too small, it will exhibit high bias as the classification model is restricted to a smaller hypothesis space. In an attempt to balance these two factors, I plotted model complexity curves below.  This was done using 5-fold cross validation.  Each training set was broken into 5 folds, then each set of 4 folds was used to train a model which is then tested against the 4 training folds, and the 5th hold out fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"DT_complexity_curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity Curve Analysis\n",
    "The learning curves showed both similarities and differences.  The HTRU2 dataset had very high accuracy on both the training and the cross validation set even with a small max depth value.  This was not a surprise as ~91% of records in the dataset are negative examples, even an algorithm that assigns every record to the negative class would be expected to have a 91% accuracy.  This is a high baseline, and decision tree algorithms could improve on this baseline, even with a small depth limit.  The letter recognition dataset showed very low accuracy with small max depth values, and drastically increased as the max depth was increased.  There are many more target classes, so I would expect it to take more branches in the decision tree to create a satisfactory model to group into one of the 26 classes.\n",
    "\n",
    "Despite the difference in initial behavior, the datasets showed similarities as the max depth increased.  Accuracy on the training sets for each approached 1.  As the decision trees are given more freedom to grow larger and larger, they can completely, or almost completely model the training data.  While this seems like a good result, the complexity curves reflect that there is a cost.  Although the bias is reduced, increasing the max depth hyperparameter increases variance.  The tree becomes so dependant on the training set that it exhibits overfitting, and ability to predict classes in the hold out set is reduced.  This is clearly seen in the HTRU2 complexity curve, as the accuracy against the validation set increases at first, but then steadily declines.  While not as obvious in the letter recognition set, it is apparent that at some point adding to the max depth does not increase the accuracy on the held out validation set.\n",
    "\n",
    "## Optimizing Hyperparameters\n",
    "After analysis of the complexity curves, the next step was to optimize the hyperparameter of max depth.  This was done using a Grid Search method.  This method is given a set of possible hyperparemeter values, and for each possible value conducts kfold validation.  The results of this analysis can be used to find the \"best\" values of the hyperparameter, and to train a model using the best value.  After running each of my training sets through this grid search process (using 5-fold validation) I determined that the optimal max depth parameters for the HTRU2 and Letter Recognition datasets were 4 and 44, respectively.  This reflected my analysis of the complexity curves.\n",
    "\n",
    "## Learning Curves\n",
    "The final step in Decision Tree analysis was to plot the learning curves.  A learning curve shows how the size of the training set may affect the model's accuracy on both the training set and a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Letters_DT_LC.png\"><img src=\"HTRU_DT_LC.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "The final step was to use my models to predict the class of the test sets, and compare the predicted classes to the actual classes.  The HTRU2 model was able to predict the correct class in 97.7% of the test cases.  The Letter Recognition model was able to predict the correct class for 86.8% of the test cases.  While this might indicate that the HTRU2 model was better, the nature of the datasets prevents direct comparision.  As mentioned previously, a simple algorithm that predicts a negative class for any attribute values would in fact be expected to predict 91% of test cases accurately.  To visualize this, I created a confusion matrix for the HTRU2 model, below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='HTRU_DT_conf_mat.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model was successful in >99% of negative test cases, it was not as successful predicting positve cases at around 82%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
